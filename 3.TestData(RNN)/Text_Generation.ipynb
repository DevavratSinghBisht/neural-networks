{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pQ85mqOw7SD",
        "outputId": "7467d0a4-2cc0-4ef7-a81b-6bb72a2dbe83"
      },
      "source": [
        "# checking GPU connection\r\n",
        "# you can skip this cell\r\n",
        "import tensorflow as tf\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "print('Found GPU at: {}'.format(device_name))\r\n",
        "tf.device(device_name)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.context._EagerDeviceContext at 0x7fdfcf86f908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aDOe3dxSOAm",
        "outputId": "206ee363-be4a-4372-bcff-a4099da31771"
      },
      "source": [
        "# connecting my drive\r\n",
        "# you can skip this cell\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPSVPLfu4r_j"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gIgMwCJu1Pq"
      },
      "source": [
        "import numpy as np\r\n",
        "from tensorflow.keras.layers import Dense, Activation\r\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\r\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfMLTcq14xso"
      },
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_8WijpC7zAk"
      },
      "source": [
        "Explanation of code below:\r\n",
        "* 'lines' is a list that will contain each line\r\n",
        "* the text file is opened and given the variable name '_in'\r\n",
        "* 'line' iterates through each line in the text document\r\n",
        "* strip() function removes any spaces before and after the line\r\n",
        "* lower() function converts each character into lowercase\r\n",
        "* decode() function has to parameters\r\n",
        "    * 'ascii' represnts the desired encoding\r\n",
        "    * 'ignore' is used to ignore the character and continue with the next if an error arises because of that character.\r\n",
        "* if length of a line is zero then it is not appended to the list 'lines'\r\n",
        "* the loop continues till the end of the document\r\n",
        "* 'text' is a single string that contais the whole data of the document\r\n",
        "* 'text' can be seen as concatination of all the srtrings present in the list 'lines'\r\n",
        "* 'char' is the set of all the characters in the document\r\n",
        "* 'no_chars' is the number of unique characters in the document "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_152geGvFAz"
      },
      "source": [
        "# Read lines from an example source file.\r\n",
        "# enter the path to your text file below if you are running a copy of this notebook\r\n",
        "with open(\"/content/drive/MyDrive/Study/DL/NLP/Text Generation/alice_in_wonderland.txt\", 'rb') as _in:\r\n",
        "    lines = []\r\n",
        "    for line in _in:\r\n",
        "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\r\n",
        "        if len(line) == 0:\r\n",
        "            continue\r\n",
        "        lines.append(line)\r\n",
        "text = \" \".join(lines)\r\n",
        "chars = set([c for c in text])\r\n",
        "no_chars = len(chars)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5nlQsSNEZYU"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghu_PyBn-WsS"
      },
      "source": [
        "Create a character index and reverse mapping to go between a numerical ID and a specific character. \r\n",
        "\r\n",
        "The numerical ID will correspond to a column number when using a one-hot encoded representation of character inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdSKQMCfvHaW"
      },
      "source": [
        "char2index = {c: i for i, c in enumerate(chars)}\r\n",
        "index2char = {i: c for i, c in enumerate(chars)}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeyCOkYtB4i1"
      },
      "source": [
        "Now we will create a 2 lists\r\n",
        "* A list of input sequence as 'input_chars'\r\n",
        "* List of corresponding next character for that sequence as 'label_chars'\r\n",
        "\r\n",
        "For convenience, I chose a fixed sequence length of 10 characters you can chose "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g1lXvEBvLtb"
      },
      "source": [
        "SEQLEN, STEP = 10, 1\r\n",
        "input_chars, label_chars = [], []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7Z-UyfyvOfg"
      },
      "source": [
        "# Convert the data into a series of different SEQLEN-length subsequences.\r\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\r\n",
        "    input_chars.append(text[i: i + SEQLEN])\r\n",
        "    label_chars.append(text[i + SEQLEN])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twelaAb6Dr-W"
      },
      "source": [
        "Computing one-hot encoding of the input sequences X and the next character i.e. the label (y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4faVhDs_vQ3O"
      },
      "source": [
        "X = np.zeros((len(input_chars), SEQLEN, no_chars), dtype=np.bool)\r\n",
        "y = np.zeros((len(input_chars), no_chars), dtype=np.bool)\r\n",
        "for i, input_char in enumerate(input_chars):\r\n",
        "    for j, ch in enumerate(input_char):\r\n",
        "        X[i, j, char2index[ch]] = 1\r\n",
        "    y[i, char2index[label_chars[i]]] = 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZJGt9g8ETPd"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzFtrWEZGlfZ"
      },
      "source": [
        "Now we will build a model with two GRU layers.\r\n",
        "\r\n",
        "We will train the model for few iterations and then see the results it is giving.\r\n",
        "Then we will loop the process of training and producing a new sequence for few times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ugvrf4RvUXl"
      },
      "source": [
        "# Set up a bunch of hyperparameters for the network and training.\r\n",
        "BATCH_SIZE, HIDDEN_SIZE = 128, 128\r\n",
        "NUM_ITERATIONS = 5\r\n",
        "NUM_EPOCHS_PER_ITERATION = 5\r\n",
        "\r\n",
        "# this number will specify the number of character predicted in a single iteration\r\n",
        "NUM_PREDS_PER_ITERATION = 100"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTwpo8kTHi37"
      },
      "source": [
        "Create a simple recurrent neural network.\r\n",
        "\r\n",
        "There are 2 recurrent layer that produces an embedding of size HIDDEN_SIZE. The first layer takes input from the one-hot encoded input layer, output of this layer acts as the input of second layer. This is followed by a Dense fully-connected layer across the set of possible next characters, which is converted to a probability score via a standard softmax activation with a multi-class cross-entropy loss function linking the prediction to the one-hot\r\n",
        "encoding character label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXo_R9XAvX9i",
        "outputId": "b7c976cd-4331-41c2-d984-dad235c66a90"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(\r\n",
        "    GRU(  # You can vary this with LSTM or SimpleRNN to try alternatives.\r\n",
        "        HIDDEN_SIZE,\r\n",
        "        return_sequences=True,\r\n",
        "        input_shape=(SEQLEN, no_chars),\r\n",
        "        unroll=True\r\n",
        "    )\r\n",
        ")\r\n",
        "model.add(\r\n",
        "    GRU(\r\n",
        "        HIDDEN_SIZE,\r\n",
        "        return_sequences=False,\r\n",
        "        unroll=True\r\n",
        "    )\r\n",
        ")\r\n",
        "model.add(Dense(no_chars))\r\n",
        "model.add(Activation(\"softmax\"))\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmLZQvXXSboS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebdc0f9-fa6a-41b3-9915-df8331c610f6"
      },
      "source": [
        "# Execute a series of training and demonstration iterations.\r\n",
        "for iteration in range(NUM_ITERATIONS):\r\n",
        "\r\n",
        "    # For each iteration, run the model fitting procedure for a number of epochs.\r\n",
        "    print(\"_\" * 150)\r\n",
        "    print(\"Iteration #: %d\" % (iteration))\r\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\r\n",
        "\r\n",
        "    # Select a random example input sequence.\r\n",
        "    test_idx = np.random.randint(len(input_chars))\r\n",
        "    test_chars = input_chars[test_idx]\r\n",
        "\r\n",
        "    # For a number of prediction steps using the current version of the trained model\r\n",
        "    # construct a one-hot encoding of the test input and append a prediction.\r\n",
        "    print(\"Generating from seed: %s\" % (test_chars))\r\n",
        "    print(test_chars, end=\"\")\r\n",
        "    for i in range(NUM_PREDS_PER_ITERATION):\r\n",
        "\r\n",
        "        # Here is the one-hot encoding.\r\n",
        "        X_test = np.zeros((1, SEQLEN, no_chars))\r\n",
        "        for j, ch in enumerate(test_chars):\r\n",
        "            X_test[0, j, char2index[ch]] = 1\r\n",
        "\r\n",
        "        # Make a prediction with the current model.\r\n",
        "        pred = model.predict(X_test, verbose=0)[0]\r\n",
        "        y_pred = index2char[np.argmax(pred)]\r\n",
        "\r\n",
        "        # Print the prediction appended to the test example.\r\n",
        "        print(y_pred, end=\"\")\r\n",
        "\r\n",
        "        # Increment the test example to contain the prediction as if it\r\n",
        "        # were the correct next letter.\r\n",
        "        test_chars = test_chars[1:] + y_pred\r\n",
        "    print()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "______________________________________________________________________________________________________________________________________________________\n",
            "Iteration #: 0\n",
            "Epoch 1/5\n",
            "1241/1241 [==============================] - 14s 9ms/step - loss: 2.5209\n",
            "Epoch 2/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.7545\n",
            "Epoch 3/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.5477\n",
            "Epoch 4/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.4302\n",
            "Epoch 5/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.3486\n",
            "Generating from seed: may charge\n",
            "may charge the footman in the first the footman in the first the footman in the first the footman in the first\n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "Iteration #: 1\n",
            "Epoch 1/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.2801\n",
            "Epoch 2/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.2328\n",
            "Epoch 3/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.1932\n",
            "Epoch 4/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.1574\n",
            "Epoch 5/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.1235\n",
            "Generating from seed: : she foun\n",
            ": she found the whole party at the trials were all the trials were all the trials were all the trials were all\n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "Iteration #: 2\n",
            "Epoch 1/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.0940\n",
            "Epoch 2/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.0652\n",
            "Epoch 3/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.0398\n",
            "Epoch 4/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 1.0144\n",
            "Epoch 5/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.9911\n",
            "Generating from seed: ame solemn\n",
            "ame solemnly, and the mouse only a thing to the caterpillar to do that it makes the cook to the project gutenb\n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "Iteration #: 3\n",
            "Epoch 1/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.9675\n",
            "Epoch 2/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.9474\n",
            "Epoch 3/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.9272\n",
            "Epoch 4/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.9064\n",
            "Epoch 5/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.8875\n",
            "Generating from seed: en she not\n",
            "en she noticed that it might happened to herself, i dont think it would be a candle is bliggled when she was g\n",
            "______________________________________________________________________________________________________________________________________________________\n",
            "Iteration #: 4\n",
            "Epoch 1/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.8695\n",
            "Epoch 2/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.8532\n",
            "Epoch 3/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.8366\n",
            "Epoch 4/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.8230\n",
            "Epoch 5/5\n",
            "1241/1241 [==============================] - 11s 9ms/step - loss: 0.8071\n",
            "Generating from seed: he had bee\n",
            "he had been for this moment, the mock turtle said to herself, and the moral of that is--be a concking that it \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kno7Ko8oIpHb"
      },
      "source": [
        "We can see that some sensible words are being generated although the texts are starting to repeat itslef after some time. Considering the number of epochs and the depth of network, this is pretty impressive."
      ]
    }
  ]
}