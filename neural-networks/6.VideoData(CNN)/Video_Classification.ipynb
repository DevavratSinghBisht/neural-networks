{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4jKYSY6dGhovd/SzBUfnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevavratSinghBisht/neural-networks/blob/main/neural-networks/6.VideoData(CNN)/Video_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pfKga_ztqE0"
      },
      "source": [
        "# Video Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdsBAOLY7pj7"
      },
      "source": [
        "* @author: Devavrat Singh Bisht\r\n",
        "* Dataset: YouTube DataSet Annotated\r\n",
        "* Click [here](https://www.crcv.ucf.edu/data/YouTube_DataSet_Annotated.zip) to download the whole YouTube DataSet.\r\n",
        "* Click [here](https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php) to visit the website where you can download this and many other similar datasets.\r\n",
        "* Note: I have reduced the dataset to the 3 classes mentioned below, in order to reduce the dataset size and thus computation required in order to fit it. The original dataset contains 11 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaDqqV2W4emj"
      },
      "source": [
        "In this session we will do video classification.\r\n",
        "There are 3 classes/types of videos:\r\n",
        "* Walking\r\n",
        "* Horse Riding\r\n",
        "* Bikinng\r\n",
        "\r\n",
        "As a video is made up of frames, we will take multiple frames from a single video and make a convolutional network using Conv3D laeyers to predict the class of the video.\r\n",
        "\r\n",
        "The video in our dataset are small and is about 10sec on an average. So taking 5 frames from the video seems good enough for our learning purpose, as we do not have access to high computation.\r\n",
        "\r\n",
        "Also building a model that perfectly fits a video data needs a huge dataset and a lot of computation. Thus, understanding the concept is our main aim in this notbook, none the less we will also try to optimize the model a little bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNTw0tEI9Fif"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FM6OlU69frv",
        "outputId": "08b5e83e-a7ad-4591-eeb4-96f222ef5877"
      },
      "source": [
        "# you can ignore this\r\n",
        "# connecting to drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2kmJfYz9JCj"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Conv3D, Flatten, Dense, Dropout, BatchNormalization\r\n",
        "\r\n",
        "import os\r\n",
        "import cv2\r\n",
        "import random\r\n",
        "import numpy as np"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT-Ogs8F8-aG"
      },
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiQQZthv2qqT"
      },
      "source": [
        "We will create a data generator or you can also call it as data loader, this class will take the video data directly from the hard drive and get exactly 5 frames from the video and since each frame will be resized to (32, 32, 3) irrespective of the shape of original frames in the video, we will get an output of shape (batch size, 5, 32, 32, 3).\r\n",
        "\r\n",
        "One 4D array will represent 1 Video. And stacking all these 4D arrays to make a batch will result in a 5D array of whole dataset.\r\n",
        "As we only have 3 classes our target variable will be of shape (batch size, 3) that is a 2D array as it usually is. The only change that happens here is in the independaent variable i.e. X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-m9QmZa9B39"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\r\n",
        "  'Generates data for Keras'\r\n",
        "  def __init__(self, dataset_path, batch_size=32, dim=(5, 32, 32, 3), vid_per_class = 21*3):\r\n",
        "    self.dataset_path = dataset_path\r\n",
        "    self.dir_list = os.listdir(dataset_path)\r\n",
        "    self.n_classes = len(self.dir_list)\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.dim = dim\r\n",
        "    self.frame_per_vid, self.height, self.width, self.channels = self.dim\r\n",
        "    self.dataset_len = 0\r\n",
        "    self.vid_per_class = vid_per_class\r\n",
        "\r\n",
        "    # for dir in self.dir_list:\r\n",
        "    #   dir_path = self.dataset_path + '/' + dir\r\n",
        "    #   #print(os.listdir(dir_path))\r\n",
        "    #   self.dataset_len = self.dataset_len + len(os.listdir(dir_path))\r\n",
        "\r\n",
        "    # self.dataset_len = self.dataset_len * self.dim[0]\r\n",
        "\r\n",
        "    self.dataset_len = self.n_classes * self. vid_per_class\r\n",
        "      \r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    'Denotes the number of batches per epoch'\r\n",
        "    return int(np.floor(self.dataset_len / self.batch_size))\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    'Generate one batch of data'\r\n",
        "    # Generate data\r\n",
        "    X, y = self.__data_generation()\r\n",
        "\r\n",
        "    return X, y\r\n",
        "\r\n",
        "  def __data_generation(self):\r\n",
        "    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\r\n",
        "    # Initialization\r\n",
        "    X = np.zeros((self.batch_size, *self.dim))\r\n",
        "    y = np.zeros((self.batch_size, self.n_classes), dtype=int)\r\n",
        "\r\n",
        "    # Generate data\r\n",
        "    for i in range(self.batch_size):\r\n",
        "      #print(i)\r\n",
        "\r\n",
        "      frame_list = []\r\n",
        "\r\n",
        "      #generates random number between and inclusive of the limiting values\r\n",
        "      class_no = random.randint(0, self.n_classes-1)\r\n",
        "\r\n",
        "      vid_dir_path = self.dataset_path + \"//\" + self.dir_list[class_no]\r\n",
        "      vid_path = vid_dir_path + \"//\" + random.choice(os.listdir(vid_dir_path))\r\n",
        "\r\n",
        "      cam = cv2.VideoCapture(vid_path)\r\n",
        "\r\n",
        "      currentframe = 0\r\n",
        "  \r\n",
        "      while(True): \r\n",
        "      \r\n",
        "        # reading from frame \r\n",
        "        ret,frame = cam.read() \r\n",
        "  \r\n",
        "        if ret:\r\n",
        "          frame = cv2.resize(frame, (self.height, self.width), interpolation = cv2.INTER_NEAREST)\r\n",
        "          frame_list.append(frame)           \r\n",
        "        else: \r\n",
        "          break\r\n",
        "      \r\n",
        "      multiplier = (len(frame_list)-1)//(self.frame_per_vid-1)\r\n",
        "\r\n",
        "      for j in range(self.frame_per_vid):\r\n",
        "        #print(j, multiplier, len(frame_list), frame_list[j*multiplier].shape)\r\n",
        "        X[i, j, :, :, :] = frame_list[j*multiplier]\r\n",
        "\r\n",
        "      y[i, class_no] = 1 \r\n",
        "\r\n",
        "    X = X/255\r\n",
        "\r\n",
        "    return X, y"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkQYV5Lg96c0"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARy-hiJVKv84"
      },
      "source": [
        "model = Sequential([\r\n",
        "                    Conv3D(4, kernel_size=(2, 8, 8), input_shape=(5, 32, 32, 3), activation='relu'),\r\n",
        "                    Dropout(0.5), # for regularization \r\n",
        "                    Conv3D(16, kernel_size=(2, 8, 8), activation='relu'),\r\n",
        "                    Dropout(0.5),\r\n",
        "                    Conv3D(32, kernel_size=(1, 16, 16), activation='relu'),\r\n",
        "                    Dropout(0.5),\r\n",
        "                    Flatten(),\r\n",
        "                    Dense(256, activation='relu'),\r\n",
        "                    Dense(3, activation='softmax')\r\n",
        "])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb0B13PYMinW"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkVuLXH5jYFa",
        "outputId": "210e6e9e-7d5f-467d-952e-1401b2ae6216"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_11 (Conv3D)           (None, 4, 25, 25, 4)      1540      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4, 25, 25, 4)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 4, 25, 25, 4)      16        \n",
            "_________________________________________________________________\n",
            "conv3d_12 (Conv3D)           (None, 3, 18, 18, 16)     8208      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 3, 18, 18, 16)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 3, 18, 18, 16)     64        \n",
            "_________________________________________________________________\n",
            "conv3d_13 (Conv3D)           (None, 3, 3, 3, 32)       131104    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 3, 3, 32)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 3, 32)       128       \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 864)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               221440    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 363,271\n",
            "Trainable params: 363,167\n",
            "Non-trainable params: 104\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne3LZemZ9-_s"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C13OufbGOSLU"
      },
      "source": [
        "train_datagen = DataGenerator('/content/drive/MyDrive/Study/DL/Video Classification/Datset/Train')\r\n",
        "val_datagen = DataGenerator('/content/drive/MyDrive/Study/DL/Video Classification/Datset/Val', batch_size=4, vid_per_class= 2*3)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywrq1nKaoncx",
        "outputId": "9a8e07d9-45da-498c-f183-6d75c035888b"
      },
      "source": [
        "model.fit(train_datagen, validation_data=val_datagen, epochs=50)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 13s 2s/step - loss: 1.5414 - accuracy: 0.2614 - val_loss: 1.0925 - val_accuracy: 0.4375\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 1.3740 - accuracy: 0.3453 - val_loss: 1.1241 - val_accuracy: 0.2500\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 1.3019 - accuracy: 0.3822 - val_loss: 1.0726 - val_accuracy: 0.3750\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 1.1446 - accuracy: 0.4460 - val_loss: 1.0357 - val_accuracy: 0.3125\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 1.4539 - accuracy: 0.3799 - val_loss: 1.0735 - val_accuracy: 0.4375\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 1.0638 - accuracy: 0.5286 - val_loss: 1.0225 - val_accuracy: 0.5625\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 1.1394 - accuracy: 0.4753 - val_loss: 1.1680 - val_accuracy: 0.4375\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 1.0286 - accuracy: 0.5581 - val_loss: 0.8197 - val_accuracy: 0.6875\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.9903 - accuracy: 0.6144 - val_loss: 1.2041 - val_accuracy: 0.1250\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.9463 - accuracy: 0.5266 - val_loss: 1.0445 - val_accuracy: 0.6250\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.8446 - accuracy: 0.5685 - val_loss: 1.0525 - val_accuracy: 0.5625\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 1.0205 - accuracy: 0.5784 - val_loss: 0.8561 - val_accuracy: 0.8125\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.7121 - accuracy: 0.7214 - val_loss: 0.8732 - val_accuracy: 0.7500\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.7307 - accuracy: 0.7023 - val_loss: 1.2226 - val_accuracy: 0.3750\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.8296 - accuracy: 0.6510 - val_loss: 1.0731 - val_accuracy: 0.4375\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.8410 - accuracy: 0.5957 - val_loss: 1.4926 - val_accuracy: 0.4375\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.7421 - accuracy: 0.6712 - val_loss: 1.3048 - val_accuracy: 0.3125\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.8641 - accuracy: 0.5986 - val_loss: 2.0666 - val_accuracy: 0.1875\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.6133 - accuracy: 0.7633 - val_loss: 1.9804 - val_accuracy: 0.1875\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.5998 - accuracy: 0.7011 - val_loss: 1.4105 - val_accuracy: 0.3750\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.5753 - accuracy: 0.7777 - val_loss: 1.9544 - val_accuracy: 0.1250\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.6515 - accuracy: 0.7361 - val_loss: 1.4430 - val_accuracy: 0.2500\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.4803 - accuracy: 0.8385 - val_loss: 1.5371 - val_accuracy: 0.3125\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.5466 - accuracy: 0.7519 - val_loss: 1.4250 - val_accuracy: 0.3750\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.4752 - accuracy: 0.7697 - val_loss: 1.4380 - val_accuracy: 0.3750\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.4542 - accuracy: 0.8277 - val_loss: 1.3864 - val_accuracy: 0.3750\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.5404 - accuracy: 0.7861 - val_loss: 1.8454 - val_accuracy: 0.5000\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.5545 - accuracy: 0.7891 - val_loss: 2.1006 - val_accuracy: 0.1875\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.5125 - accuracy: 0.7840 - val_loss: 1.7419 - val_accuracy: 0.3750\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.4994 - accuracy: 0.8150 - val_loss: 2.5810 - val_accuracy: 0.3125\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.4718 - accuracy: 0.7419 - val_loss: 3.3582 - val_accuracy: 0.2500\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.4054 - accuracy: 0.8659 - val_loss: 2.5017 - val_accuracy: 0.4375\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.3628 - accuracy: 0.8343 - val_loss: 3.4525 - val_accuracy: 0.2500\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.3310 - accuracy: 0.8725 - val_loss: 2.6851 - val_accuracy: 0.1250\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.3043 - accuracy: 0.9113 - val_loss: 1.9757 - val_accuracy: 0.5000\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.3871 - accuracy: 0.8703 - val_loss: 1.0889 - val_accuracy: 0.7500\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.6304 - accuracy: 0.7635 - val_loss: 1.4033 - val_accuracy: 0.6875\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.3333 - accuracy: 0.8757 - val_loss: 2.9047 - val_accuracy: 0.3750\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.4310 - accuracy: 0.8628 - val_loss: 4.3339 - val_accuracy: 0.4375\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.3731 - accuracy: 0.8808 - val_loss: 3.0875 - val_accuracy: 0.1250\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.3082 - accuracy: 0.8779 - val_loss: 1.4481 - val_accuracy: 0.4375\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.3793 - accuracy: 0.8336 - val_loss: 2.3376 - val_accuracy: 0.3750\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.3372 - accuracy: 0.8644 - val_loss: 1.8538 - val_accuracy: 0.6250\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.3769 - accuracy: 0.8298 - val_loss: 2.4137 - val_accuracy: 0.3125\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.2952 - accuracy: 0.8911 - val_loss: 2.0565 - val_accuracy: 0.5000\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.3350 - accuracy: 0.8831 - val_loss: 3.2865 - val_accuracy: 0.5000\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.2629 - accuracy: 0.8811 - val_loss: 2.3532 - val_accuracy: 0.6250\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.2521 - accuracy: 0.8939 - val_loss: 4.0100 - val_accuracy: 0.4375\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.2312 - accuracy: 0.8995 - val_loss: 4.2593 - val_accuracy: 0.4375\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 0.3197 - accuracy: 0.8643 - val_loss: 2.3358 - val_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4660668e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}