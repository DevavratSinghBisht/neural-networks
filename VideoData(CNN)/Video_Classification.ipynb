{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzhVmPWxSAEgjRUAWipTT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevavratSinghBisht/neural-networks/blob/main/VideoData(CNN)/Video_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pfKga_ztqE0"
      },
      "source": [
        "# Video Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdsBAOLY7pj7"
      },
      "source": [
        "* @author: Devavrat Singh Bisht\r\n",
        "* Dataset: YouTube DataSet Annotated\r\n",
        "* Click [here](https://www.crcv.ucf.edu/data/YouTube_DataSet_Annotated.zip) to download the whole YouTube DataSet.\r\n",
        "* Click [here](https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php) to visit the website where you can download this and many other similar datasets.\r\n",
        "* Note: I have reduced the dataset to the 3 classes mentioned below, in order to reduce the dataset size and thus computation required in order to fit it. The original dataset contains 11 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaDqqV2W4emj"
      },
      "source": [
        "In this session we will do video classification.\r\n",
        "There are 3 classes/types of videos:\r\n",
        "* Walking\r\n",
        "* Horse Riding\r\n",
        "* Bikinng\r\n",
        "\r\n",
        "As a video is made up of frames, we will take multiple frames from a single video and make a convolutional network using Conv3D laeyers to predict the class of the video.\r\n",
        "\r\n",
        "The video in our dataset are small and is about 10sec on an average. So taking 5 frames from the video seems good enough for our learning purpose, as we do not have access to high computation.\r\n",
        "\r\n",
        "Also building a model that perfectly fits a video data needs a huge dataset and a lot of computation. Thus, understanding the concept is our main aim in this notbook, none the less we will also try to optimize the model a little bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNTw0tEI9Fif"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FM6OlU69frv",
        "outputId": "35fc11b3-3d34-4a29-90dd-8906416bb7ff"
      },
      "source": [
        "# you can ignore this\r\n",
        "# connecting to drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2kmJfYz9JCj"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Conv3D, Flatten, Dense, Dropout, BatchNormalization, Input, ConvLSTM2D\r\n",
        "\r\n",
        "import os\r\n",
        "import cv2\r\n",
        "import random\r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT-Ogs8F8-aG"
      },
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiQQZthv2qqT"
      },
      "source": [
        "We will create a data generator or you can also call it as data loader, this class will take the video data directly from the hard drive and get exactly 5 frames from the video and since each frame will be resized to (32, 32, 3) irrespective of the shape of original frames in the video, we will get an output of shape (batch size, 5, 32, 32, 3).\r\n",
        "\r\n",
        "One 4D array will represent 1 Video. And stacking all these 4D arrays to make a batch will result in a 5D array of whole dataset.\r\n",
        "As we only have 3 classes our target variable will be of shape (batch size, 3) that is a 2D array as it usually is. The only change that happens here is in the independaent variable i.e. X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-m9QmZa9B39"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\r\n",
        "  'Generates data for Keras'\r\n",
        "  def __init__(self, dataset_path, batch_size=32, dim=(5, 32, 32, 3), vid_per_class = 21*3):\r\n",
        "    self.dataset_path = dataset_path\r\n",
        "    self.dir_list = os.listdir(dataset_path)\r\n",
        "    self.n_classes = len(self.dir_list)\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.dim = dim\r\n",
        "    self.frame_per_vid, self.height, self.width, self.channels = self.dim\r\n",
        "    self.dataset_len = 0\r\n",
        "    self.vid_per_class = vid_per_class\r\n",
        "    self.dataset_len = self.n_classes * self. vid_per_class\r\n",
        "      \r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    'Denotes the number of batches per epoch'\r\n",
        "    return int(np.floor(self.dataset_len / self.batch_size))\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    'Generate one batch of data'\r\n",
        "    # Generate data\r\n",
        "    X, y = self.__data_generation()\r\n",
        "\r\n",
        "    return X, y\r\n",
        "\r\n",
        "  def __data_generation(self):\r\n",
        "    'Generates data containing batch_size samples' # X : (n_samples, n_channels, *dim)\r\n",
        "    # Initialization\r\n",
        "    X = np.zeros((self.batch_size, *self.dim))\r\n",
        "    y = np.zeros((self.batch_size, self.n_classes), dtype=int)\r\n",
        "\r\n",
        "    # Generate data\r\n",
        "    for i in range(self.batch_size):\r\n",
        "      #print(i)\r\n",
        "\r\n",
        "      frame_list = []\r\n",
        "\r\n",
        "      #generates random number between and inclusive of the limiting values\r\n",
        "      class_no = random.randint(0, self.n_classes-1)\r\n",
        "\r\n",
        "      vid_dir_path = self.dataset_path + \"//\" + self.dir_list[class_no]\r\n",
        "      vid_path = vid_dir_path + \"//\" + random.choice(os.listdir(vid_dir_path))\r\n",
        "\r\n",
        "      cam = cv2.VideoCapture(vid_path)\r\n",
        "\r\n",
        "      currentframe = 0\r\n",
        "  \r\n",
        "      while(True): \r\n",
        "      \r\n",
        "        # reading from frame \r\n",
        "        ret,frame = cam.read() \r\n",
        "  \r\n",
        "        if ret:\r\n",
        "          frame = cv2.resize(frame, (self.height, self.width), interpolation = cv2.INTER_NEAREST)\r\n",
        "          frame_list.append(frame)           \r\n",
        "        else: \r\n",
        "          break\r\n",
        "      \r\n",
        "      multiplier = (len(frame_list)-1)//(self.frame_per_vid-1)\r\n",
        "\r\n",
        "      for j in range(self.frame_per_vid):\r\n",
        "        #print(j, multiplier, len(frame_list), frame_list[j*multiplier].shape)\r\n",
        "        X[i, j, :, :, :] = frame_list[j*multiplier]\r\n",
        "\r\n",
        "      y[i, class_no] = 1 \r\n",
        "\r\n",
        "    X = X/255\r\n",
        "\r\n",
        "    return X, y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kJKorXqtN1K"
      },
      "source": [
        "## Model1\r\n",
        "Here we will build a model using Conv3D layers, it is similar to Conv2D layers but it does convolution operation in three dimensions rather than the conventional two dimension of the images.\r\n",
        "\r\n",
        "What we are goning to do is stack the frames of the movie one over the other making a 3D cube of arrays (note that the dimension of channel is ignored here), after forming the 3D cube we will apply some Conv3D layers followed by a flatten and some dense layers. \r\n",
        "\r\n",
        "The model may not be good enough as the data set it very small considering that we are working on videos. But it's sufficient for our learning purposes. The model overfits but I believe that give more data and computing resources it will result in higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkQYV5Lg96c0"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARy-hiJVKv84"
      },
      "source": [
        "model1 = Sequential([\r\n",
        "                     Conv3D(4, kernel_size=(2, 8, 8), input_shape=(5, 32, 32, 3), activation='relu'),\r\n",
        "                     Conv3D(16, kernel_size=(2, 8, 8), activation='relu'),\r\n",
        "                     Conv3D(32, kernel_size=(1, 16, 16), activation='relu'),\r\n",
        "                     Flatten(),\r\n",
        "                     Dense(256, activation='relu'),\r\n",
        "                     Dense(3, activation='softmax')\r\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb0B13PYMinW"
      },
      "source": [
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkVuLXH5jYFa",
        "outputId": "d34e436b-a7a0-45a0-96cf-4fbf6fed9084"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 4, 25, 25, 4)      1540      \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 3, 18, 18, 16)     8208      \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 3, 3, 3, 32)       131104    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 864)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               221440    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 363,063\n",
            "Trainable params: 363,063\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne3LZemZ9-_s"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C13OufbGOSLU"
      },
      "source": [
        "train_datagen = DataGenerator('/content/drive/MyDrive/Study/DL/Video Classification/Datset/Train')\r\n",
        "val_datagen = DataGenerator('/content/drive/MyDrive/Study/DL/Video Classification/Datset/Val', batch_size=4, vid_per_class= 2*3)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywrq1nKaoncx",
        "outputId": "fd932445-1bbf-4d78-9456-85de13c9de40"
      },
      "source": [
        "model1.fit(train_datagen, validation_data=val_datagen, epochs=50)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 11s 2s/step - loss: 1.1931 - accuracy: 0.3935 - val_loss: 1.0748 - val_accuracy: 0.4375\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.0946 - accuracy: 0.3896 - val_loss: 1.0995 - val_accuracy: 0.3750\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.0697 - accuracy: 0.4543 - val_loss: 1.1016 - val_accuracy: 0.3750\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.0837 - accuracy: 0.4596 - val_loss: 1.0892 - val_accuracy: 0.2500\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.0862 - accuracy: 0.4325 - val_loss: 1.0903 - val_accuracy: 0.3125\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.0905 - accuracy: 0.3839 - val_loss: 1.0512 - val_accuracy: 0.5625\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.0709 - accuracy: 0.4276 - val_loss: 1.1110 - val_accuracy: 0.3125\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.0926 - accuracy: 0.4390 - val_loss: 1.0206 - val_accuracy: 0.3750\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.9922 - accuracy: 0.4873 - val_loss: 1.0856 - val_accuracy: 0.3750\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.9601 - accuracy: 0.4891 - val_loss: 0.9410 - val_accuracy: 0.5625\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.0530 - accuracy: 0.5002 - val_loss: 0.9412 - val_accuracy: 0.6250\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.9628 - accuracy: 0.5044 - val_loss: 1.1193 - val_accuracy: 0.2500\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8761 - accuracy: 0.6308 - val_loss: 0.8921 - val_accuracy: 0.5000\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.9620 - accuracy: 0.5000 - val_loss: 0.8406 - val_accuracy: 0.5625\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8985 - accuracy: 0.5363 - val_loss: 1.1700 - val_accuracy: 0.3125\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8301 - accuracy: 0.5701 - val_loss: 1.2776 - val_accuracy: 0.2500\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7987 - accuracy: 0.5521 - val_loss: 1.2782 - val_accuracy: 0.3125\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8029 - accuracy: 0.6149 - val_loss: 0.9795 - val_accuracy: 0.4375\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.9727 - accuracy: 0.4392 - val_loss: 0.8657 - val_accuracy: 0.6250\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.9189 - accuracy: 0.5085 - val_loss: 0.9018 - val_accuracy: 0.4375\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8672 - accuracy: 0.5941 - val_loss: 1.1974 - val_accuracy: 0.2500\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8170 - accuracy: 0.5411 - val_loss: 0.8501 - val_accuracy: 0.6250\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7390 - accuracy: 0.6521 - val_loss: 1.0232 - val_accuracy: 0.4375\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8432 - accuracy: 0.5094 - val_loss: 1.0551 - val_accuracy: 0.4375\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8274 - accuracy: 0.6122 - val_loss: 1.1242 - val_accuracy: 0.4375\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7794 - accuracy: 0.6397 - val_loss: 0.9836 - val_accuracy: 0.3125\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8288 - accuracy: 0.5761 - val_loss: 1.3752 - val_accuracy: 0.3750\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7017 - accuracy: 0.7456 - val_loss: 1.3095 - val_accuracy: 0.3125\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8470 - accuracy: 0.5484 - val_loss: 1.0075 - val_accuracy: 0.2500\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7294 - accuracy: 0.7440 - val_loss: 1.1756 - val_accuracy: 0.3125\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7126 - accuracy: 0.6756 - val_loss: 1.1476 - val_accuracy: 0.3750\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7249 - accuracy: 0.7044 - val_loss: 1.1365 - val_accuracy: 0.3125\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.6245 - accuracy: 0.7514 - val_loss: 1.0413 - val_accuracy: 0.4375\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7369 - accuracy: 0.6652 - val_loss: 1.0370 - val_accuracy: 0.4375\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7044 - accuracy: 0.6723 - val_loss: 1.2919 - val_accuracy: 0.2500\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.7026 - accuracy: 0.6562 - val_loss: 1.8223 - val_accuracy: 0.5000\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.6523 - accuracy: 0.7375 - val_loss: 1.5821 - val_accuracy: 0.3750\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.4649 - accuracy: 0.8284 - val_loss: 1.6524 - val_accuracy: 0.3125\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.5102 - accuracy: 0.7695 - val_loss: 1.7766 - val_accuracy: 0.3125\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.5424 - accuracy: 0.7584 - val_loss: 1.0958 - val_accuracy: 0.4375\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.4744 - accuracy: 0.7594 - val_loss: 1.8929 - val_accuracy: 0.3125\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.4943 - accuracy: 0.7973 - val_loss: 1.8816 - val_accuracy: 0.2500\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.5185 - accuracy: 0.7675 - val_loss: 1.3205 - val_accuracy: 0.4375\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.5372 - accuracy: 0.7502 - val_loss: 2.3572 - val_accuracy: 0.3125\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.3461 - accuracy: 0.8617 - val_loss: 1.8736 - val_accuracy: 0.3125\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.3426 - accuracy: 0.8859 - val_loss: 2.8287 - val_accuracy: 0.3125\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.4203 - accuracy: 0.8378 - val_loss: 2.0837 - val_accuracy: 0.2500\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.3712 - accuracy: 0.8187 - val_loss: 1.5971 - val_accuracy: 0.2500\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.6145 - accuracy: 0.7576 - val_loss: 1.5212 - val_accuracy: 0.3125\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.4258 - accuracy: 0.8228 - val_loss: 2.1873 - val_accuracy: 0.3750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f31489dcda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk4VoLmytboN"
      },
      "source": [
        "## Model2\r\n",
        "Here we will build a model using ConvLSTM2D layers, as you might have guessed correctly it is a combination of Conv2D layer and a LSTM layer. As video is a sequence of frames its better to use layers which are suitable for sequence data, but at ther same time it consists of frames thus we needed a layer which can treat frames as a sequence of data.\r\n",
        "\r\n",
        "We will stack the frames of the movie one over the other making a 3D cube of arrays (note that the dimension of channel is ignored here), after forming the 3D cube we will apply some ConvLSTM2D layers followed by a flatten and some dense layers. \r\n",
        "\r\n",
        "A major advantage of ConvLSTM2D over Conv3D is that it treats the data as a sequence which is how the data should be treated originaly. Also we dont need to restrict the number of frames like earlier. We will implement the model for any number of frames, although the data feeded to the model will contain 5 frame per video as we have defiend earlier for our ease.\r\n",
        "\r\n",
        "The model may not be good enough as the data set it very small considering that we are working on videos. But it's sufficient for our learning purposes. The model overfits and I believe that give more data and computing resources it will result in higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wicRdBgdq3Du"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIdeuzG0pPj_"
      },
      "source": [
        "model2 = Sequential([\r\n",
        "                     Input( shape=(None, 32, 32, 3) ),  # Variable-length sequence of 32x32x3 frames\r\n",
        "                     ConvLSTM2D( filters=40, kernel_size=(3, 3), return_sequences=True ),\r\n",
        "                     ConvLSTM2D( filters=40, kernel_size=(3, 3), return_sequences=True ),\r\n",
        "                     ConvLSTM2D( filters=40, kernel_size=(3, 3), return_sequences=False ),\r\n",
        "                     Flatten(),\r\n",
        "                     Dense(256, activation='relu'),\r\n",
        "                     Dense(3, activation='softmax'),\r\n",
        "                     ])\r\n",
        "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rytQ-E-aqlQy",
        "outputId": "c308f32c-700c-4fc4-bce3-e36421f76a1b"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_lst_m2d (ConvLSTM2D)    (None, None, 30, 30, 40)  62080     \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_1 (ConvLSTM2D)  (None, None, 28, 28, 40)  115360    \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_2 (ConvLSTM2D)  (None, 26, 26, 40)        115360    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 27040)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               6922496   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 7,216,067\n",
            "Trainable params: 7,216,067\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z28h_KJ6thWu"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_Mcyy_8qwSi",
        "outputId": "a38ef859-0ff5-434b-8802-a87512d3a520"
      },
      "source": [
        "model2.fit(train_datagen, validation_data=val_datagen, epochs=50)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 15s 2s/step - loss: 1.1606 - accuracy: 0.2860 - val_loss: 1.0622 - val_accuracy: 0.4375\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 1.1183 - accuracy: 0.2840 - val_loss: 1.1183 - val_accuracy: 0.3750\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 1.0914 - accuracy: 0.3841 - val_loss: 1.0815 - val_accuracy: 0.3750\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 1.0497 - accuracy: 0.4865 - val_loss: 1.1634 - val_accuracy: 0.1875\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 1.0265 - accuracy: 0.4750 - val_loss: 1.2140 - val_accuracy: 0.1875\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.9417 - accuracy: 0.5576 - val_loss: 0.9320 - val_accuracy: 0.4375\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.7838 - accuracy: 0.6425 - val_loss: 1.2022 - val_accuracy: 0.3750\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.8833 - accuracy: 0.6095 - val_loss: 1.0140 - val_accuracy: 0.5000\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.8649 - accuracy: 0.5322 - val_loss: 1.2371 - val_accuracy: 0.4375\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8463 - accuracy: 0.5741 - val_loss: 1.2632 - val_accuracy: 0.3750\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.6761 - accuracy: 0.6892 - val_loss: 1.4270 - val_accuracy: 0.1250\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.8381 - accuracy: 0.6484 - val_loss: 1.5645 - val_accuracy: 0.1875\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.7585 - accuracy: 0.6468 - val_loss: 1.1364 - val_accuracy: 0.2500\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.7169 - accuracy: 0.6815 - val_loss: 1.3985 - val_accuracy: 0.4375\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.6600 - accuracy: 0.7787 - val_loss: 1.1446 - val_accuracy: 0.5000\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.5076 - accuracy: 0.7626 - val_loss: 1.3027 - val_accuracy: 0.4375\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.5867 - accuracy: 0.7733 - val_loss: 1.0948 - val_accuracy: 0.5625\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.5226 - accuracy: 0.7975 - val_loss: 1.6193 - val_accuracy: 0.2500\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.5038 - accuracy: 0.8076 - val_loss: 1.6044 - val_accuracy: 0.5000\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.3885 - accuracy: 0.8405 - val_loss: 2.5066 - val_accuracy: 0.3125\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.3989 - accuracy: 0.8364 - val_loss: 3.1900 - val_accuracy: 0.1875\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.4906 - accuracy: 0.7185 - val_loss: 1.2027 - val_accuracy: 0.3750\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.4002 - accuracy: 0.8405 - val_loss: 1.5808 - val_accuracy: 0.4375\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.2824 - accuracy: 0.8880 - val_loss: 2.3207 - val_accuracy: 0.3750\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.3189 - accuracy: 0.8869 - val_loss: 2.7843 - val_accuracy: 0.3750\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.2119 - accuracy: 0.9175 - val_loss: 2.1064 - val_accuracy: 0.5000\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.1659 - accuracy: 0.9177 - val_loss: 2.3363 - val_accuracy: 0.3750\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.5125 - accuracy: 0.8691 - val_loss: 2.5753 - val_accuracy: 0.5625\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.2237 - accuracy: 0.9154 - val_loss: 4.2478 - val_accuracy: 0.1250\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.2372 - accuracy: 0.9122 - val_loss: 4.2272 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.2040 - accuracy: 0.9488 - val_loss: 3.8816 - val_accuracy: 0.3125\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.1556 - accuracy: 0.9322 - val_loss: 2.4676 - val_accuracy: 0.5625\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.1158 - accuracy: 0.9726 - val_loss: 5.2063 - val_accuracy: 0.1875\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.0755 - accuracy: 0.9682 - val_loss: 5.6114 - val_accuracy: 0.0625\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.0634 - accuracy: 0.9871 - val_loss: 5.1851 - val_accuracy: 0.3750\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.0236 - accuracy: 0.9979 - val_loss: 6.5244 - val_accuracy: 0.1875\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.0724 - accuracy: 0.9786 - val_loss: 9.2124 - val_accuracy: 0.0625\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.0694 - accuracy: 0.9915 - val_loss: 4.7477 - val_accuracy: 0.5000\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.0172 - accuracy: 0.9966 - val_loss: 8.2408 - val_accuracy: 0.1875\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.0340 - accuracy: 0.9868 - val_loss: 7.6802 - val_accuracy: 0.2500\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.0186 - accuracy: 0.9979 - val_loss: 6.9406 - val_accuracy: 0.3750\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 5.9936 - val_accuracy: 0.2500\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 8.1007 - val_accuracy: 0.1875\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.0330 - accuracy: 0.9958 - val_loss: 4.9736 - val_accuracy: 0.3750\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.0271 - accuracy: 0.9889 - val_loss: 5.1555 - val_accuracy: 0.4375\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.0369 - accuracy: 0.9923 - val_loss: 6.7875 - val_accuracy: 0.3125\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.0237 - accuracy: 0.9871 - val_loss: 6.8223 - val_accuracy: 0.3750\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 7.0614 - val_accuracy: 0.3125\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 9s 2s/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 6.5912 - val_accuracy: 0.4375\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.0175 - accuracy: 0.9949 - val_loss: 4.2211 - val_accuracy: 0.6250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f30f02e3080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}